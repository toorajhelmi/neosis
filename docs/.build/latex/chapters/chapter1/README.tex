\section{Chapter 1 --- Introduction and Motivation}\label{chapter-1-introduction-and-motivation}

Artificial neural networks (ANNs) have expanded rapidly, especially with the advent of large multimodal models (LMMs). Their growing scale suggests accelerating progress toward human-level cognition. But beneath this progress lies a defining structural limitation: modern ANNs are \emph{homogeneous, static architectures}. They possess a fixed set of layers, fixed connectivity patterns, and a fixed functional form. Training adjusts weights, but the computational \textbf{structure itself never develops}. When the training process ends, the system becomes a static artifact.

This structural rigidity stands in sharp contrast to biological cognition. Computational cognitive science (CCS) emphasizes that intelligent behavior emerges from \textbf{heterogeneous, interacting subsystems}, each with its own circuitry, plasticity rules, and computational role. The human brain is not a uniform network of identical units; it contains distinct regions with different architectures, connectivity profiles, and learning mechanisms. The basal ganglia implement forms of \textbf{model-free reinforcement learning}. The prefrontal cortex supports \textbf{model-based planning} and executive control. The hippocampus enables \textbf{episodic memory} and rapid encoding. Sensory cortices develop \textbf{specialized hierarchical filters} shaped by lifelong interaction with the environment. These structures adapt at multiple timescales---from synaptic plasticity to developmental rewiring---forming an ecosystem of computation rather than a single monolithic function approximator.

Modern LLMs, despite their impressive performance, lack these properties. Transformers rely primarily on \textbf{attention-based pattern association}, with no explicit mechanism for grounded world modeling, persistent memory, or causal reasoning beyond what statistical correlation affords. Their behavior derives from optimizing a narrow predictive objective; they do not possess differentiated substructures with specialized learning rules, nor do they develop new computational motifs as they operate. As CCS shows, true cognition requires \textbf{adaptive heterogeneity} and \textbf{evolving structure}, not simply scale.

This highlights a critical limitation: without mechanisms for \textbf{structural evolution}, \textbf{self-modifying computation}, and \textbf{developmental dynamics}, no amount of parameter scaling can reproduce the open-ended adaptability seen in natural cognitive systems. The appearance of progress toward human-level intelligence is therefore narrow --- impressive within its domain, but fundamentally constrained by a fixed architecture.

Neosis is motivated by this gap. Rather than extending static architectures, it proposes a foundation in which cognitive systems are \textbf{alive in a computational sense}. The basic organism, the \emph{Neo}, is not a fixed neural network but a \textbf{self-modifying graph dynamical system}. Its nodes carry local computational rules; its edges define information flow; and its structure can reorganize over time. A Neo can create or delete nodes, change its internal functions, expand or reduce its interfaces, or divide into independent subgraphs. It evolves under energy constraints and survival dynamics, enabling development, adaptation, and emergent complexity.

By framing cognition as \textbf{continuous structural evolution}, Neosis aligns with CCS principles rather than with the traditional ANN paradigm. It incorporates heterogeneity by allowing different nodes to implement distinct computational roles. It supports specialization as subgraphs develop persistent functions shaped by experience and mutation. It enables lifelong adaptation because both parameters and structure are mutable. And it mirrors biological cognition's open-ended nature by allowing new computational motifs to arise that were never explicitly designed.

Importantly, this framework remains mathematically tractable. Each node follows a local update rule, giving rise to a well-defined global dynamical system on an evolving graph. This dual-level structure --- local computation and global evolution --- captures individual behavior while supporting system-wide analysis. It mirrors CCS's view of neural systems: local rules give rise to complex global cognition.

The vision is ambitious but grounded: the creation of \textbf{digital organisms capable of continuous, open-ended cognitive evolution}. These organisms begin as simple structures, interacting through local rules, and grow into complex computational entities capable of prediction, adaptation, memory, and self-directed change. Their behaviors are shaped by energy economies, structural mutation, competition, and environmental constraints, mirroring the principles that drive biological evolution while remaining fully digital and mathematically analyzable.

Neosis therefore offers a foundation that current ANNs and LLMs cannot provide --- a path toward systems that do not merely approximate intelligence through statistical prediction, but \textbf{live, develop, and evolve}. This is the motivation for the Neosis framework presented in this document.
